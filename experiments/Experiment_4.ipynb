{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e6394d",
   "metadata": {},
   "source": [
    "# Learning Encodes Alignment within a Protein Family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08f10f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Needed to import modules from helpers\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "import esm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from Bio import AlignIO\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Gehe einen Ordner nach oben\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from helpers import helper\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e91232",
   "metadata": {},
   "source": [
    "# Load ESM model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "082cc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kleineres Modell 'esm2_t6_8M_UR50D' zum testen\n",
    "# verwendet 36-layer Transformer trained on UniParc\" (ca. 670 Mio. Parameter ) im Paper.\n",
    "model, alphabet = esm.pretrained.esm1_t34_670M_UR50S()\n",
    "# model, alphabet = esm.pretrained.esm1_t6_43M_UR50S()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Modell auf GPU geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b16db",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2009d",
   "metadata": {},
   "source": [
    "### Hypothesis 1:\n",
    "Final hidden representations of a sequence encode information about the MSA it belongs to. Even if it never saw the MSA (Model learns to cluster sequences belonging to the same MSA together). MSA is a long expensive process which could be improved by this model.\n",
    "\n",
    "### Method:\n",
    "\n",
    "- Get Dataset (Pfam)\n",
    "- compare the distribution of cosine similarities of representations between pairs of residues that are aligned in the family’s MSA background distribution of cosine similarities between unaligned pairs of residues.\n",
    "- Compare with distributions befor learning (We need the embeddings befor pretraining (randomize model))\n",
    "From paper: -\tUsing the collection of MSAs of structurally related sequences in Pfam (48), we compare the distribution of cosine similarities of representations between pairs of residues that are aligned in the family’s MSA to a background distribution of cosine similarities between unaligned pairs of residues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b6797d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Pfam database to find PF01010...\n",
      "This may take 1-2 minutes as it searches the compressed stream.\n",
      "\n",
      "Success! Loaded Q8HUX0_9ROSA/106-348\n",
      "Sequence count: 76\n",
      "Alignment length: 344\n"
     ]
    }
   ],
   "source": [
    "# Fetch the data. PF01010 (Response regulator receiver domain)\n",
    "msa_data = helper.get_pfam_seed_by_id(\"PF01010\")\n",
    "\n",
    "if msa_data:\n",
    "    # Parse using BioPython\n",
    "    msa = AlignIO.read(StringIO(msa_data), \"stockholm\")\n",
    "\n",
    "    # Verify\n",
    "    print(f\"\\nSuccess! Loaded {msa[0].id}\")\n",
    "    print(f\"Sequence count: {len(msa)}\")\n",
    "    print(f\"Alignment length: {msa.get_alignment_length()}\")\n",
    "else:\n",
    "    print(\"Family not found in the current Pfam release.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d9b3a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id                                           sequence\n",
      "0  Q8HUX0_9ROSA/106-348  RIYLLTFEGHLN------VHFQTYSGK--KNNLF--SSISLWG--K...\n",
      "1   Q0MS80_9ROSA/77-329  RIYLLAFEGHLN------VYFQTYSGK--KSSSF--YSMSLWGKDK...\n",
      "2  Q6JXE4_9ROSA/410-658  RIYLLTFEGHLN------VYCKTYSGK--KKRSV--YSISLWG--R...\n",
      "3  Q6JX94_9ROSA/404-655  RIYLLTFEGHLN------FYFQTYSDK--KRSSV--YSISLWG--R...\n",
      "4    NU5C_PELHO/448-688  RIYLLTFEGHLN------VHFRNYSGK--RSNSF--YSISLWG--N...\n"
     ]
    }
   ],
   "source": [
    "# Assuming your alignment object is named 'alignment'\n",
    "# We extract the ID and convert the Seq object to a string\n",
    "data = [{\"id\": record.id, \"sequence\": str(record.seq)} for record in msa]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5ff8c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0   1   2   3   4   5   6   7   8   9    ... 334 335 336  \\\n",
      "Q8HUX0_9ROSA/106-348   R   I   Y   L   L   T   F   E   G   H  ...   K   L   A   \n",
      "Q0MS80_9ROSA/77-329    R   I   Y   L   L   A   F   E   G   H  ...   R   L   A   \n",
      "Q6JXE4_9ROSA/410-658   R   I   Y   L   L   T   F   E   G   H  ...   L   F   S   \n",
      "Q6JX94_9ROSA/404-655   R   I   Y   L   L   T   F   E   G   H  ...   I   L   S   \n",
      "NU5C_PELHO/448-688     R   I   Y   L   L   T   F   E   G   H  ...   G   F   A   \n",
      "\n",
      "                     337 338 339 340 341 342 343  \n",
      "Q8HUX0_9ROSA/106-348   Q   L   T   H   F   F   D  \n",
      "Q0MS80_9ROSA/77-329    E   L   T   H   F   F   D  \n",
      "Q6JXE4_9ROSA/410-658   E   I   I   N   I   L   D  \n",
      "Q6JX94_9ROSA/404-655   K   L   I   H   F   F   D  \n",
      "NU5C_PELHO/448-688     E   F   T   H   F   F   D  \n",
      "\n",
      "[5 rows x 344 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a list where every sequence string is split into a list of characters\n",
    "data = [list(str(record.seq)) for record in msa]\n",
    "\n",
    "# Pass the IDs as the index\n",
    "ids = [record.id for record in msa]\n",
    "\n",
    "df = pd.DataFrame(data, index=ids)\n",
    "\n",
    "# This results in columns 0 to 343 (since length is 344)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76efeade",
   "metadata": {},
   "source": [
    "### Prepare Data: Map MSA columns to Sequence Indices\n",
    "The model cannot handle gaps ('.' or '-'), so we must remove them.\n",
    "However, to check if residues are \"aligned\", we need to know their original column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a33b1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 76 sequences...\n",
      "First clean sequence (len 243): RIYLLTFEGHLNVHFQTYSG...\n"
     ]
    }
   ],
   "source": [
    "def clean_and_map_msa(msa):\n",
    "    clean_seqs = []\n",
    "    msa_mapping = []  # List of dictionaries: {msa_column_index: sequence_residue_index}\n",
    "\n",
    "    print(f\"Processing {len(msa)} sequences...\")\n",
    "\n",
    "    for record in msa:\n",
    "        original_seq = str(record.seq).upper()\n",
    "        clean_seq = \"\"\n",
    "        mapping = {}\n",
    "\n",
    "        seq_idx = 0\n",
    "        for msa_idx, char in enumerate(original_seq):\n",
    "            # Pfam uses '.' and '-' for gaps. We only want amino acids.\n",
    "            if char not in [\".\", \"-\"]:\n",
    "                clean_seq += char\n",
    "                # Record that this MSA column corresponds to this index in the clean sequence\n",
    "                mapping[msa_idx] = seq_idx\n",
    "                seq_idx += 1\n",
    "\n",
    "        clean_seqs.append(clean_seq)\n",
    "        msa_mapping.append(mapping)\n",
    "    return clean_seqs, msa_mapping\n",
    "\n",
    "clean_seqs, msa_mapping = clean_and_map_msa(msa)\n",
    "\n",
    "print(f\"First clean sequence (len {len(clean_seqs[0])}): {clean_seqs[0][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684f436",
   "metadata": {},
   "source": [
    "### 2. Get Representations (Forward Pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7f9aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running forward pass on TRAINED model...\n",
      "Processing 76 sequences in batches of 1...\n",
      "Running forward pass on UNTRAINED model...\n",
      "Processing 76 sequences in batches of 1...\n",
      "Forward passes complete.\n"
     ]
    }
   ],
   "source": [
    "# A. Trained Model\n",
    "def forward_pass(clean_seqs):\n",
    "    print(\"Running forward pass on TRAINED model...\")\n",
    "# Note: We use the helper, but we pass the labels=None explicitly if your helper expects it\n",
    "    with torch.no_grad():\n",
    "        token_reps_trained, batch_strs_trained = helper.get_hidden_representations(\n",
    "        model, alphabet, np.zeros_like(clean_seqs), clean_seqs\n",
    "    )\n",
    "\n",
    "# B. Untrained Model (Random Baseline)\n",
    "    print(\"Running forward pass on UNTRAINED model...\")\n",
    "# Create a randomized version of the model to test the \"Emergence\" hypothesis\n",
    "    untrained_model = helper.randomize_model(model)\n",
    "    if torch.cuda.is_available():\n",
    "        untrained_model = untrained_model.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_reps_untrained, batch_strs_untrained = helper.get_hidden_representations(\n",
    "        untrained_model, alphabet, np.zeros_like(clean_seqs), clean_seqs\n",
    "    )\n",
    "    return token_reps_trained, token_reps_untrained\n",
    "\n",
    "\n",
    "token_reps_trained, token_reps_untrained = forward_pass(clean_seqs)\n",
    "\n",
    "print(\"Forward passes complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1934d9",
   "metadata": {},
   "source": [
    "### 3. Calculate Cosine Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbb16388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarities for TRAINED model...\n",
      "Calculating similarities for UNTRAINED model...\n",
      "Done. Collected 50000 aligned and 50000 unaligned pairs.\n",
      "train_aligned_sim[:5]=[0.9617339968681335, 0.9742878079414368, 0.9269975423812866, 0.9325844645500183, 0.9628332257270813], \n",
      "train_unaligned_sim[:5]=[0.6962365508079529, 0.7159315347671509, 0.9189531803131104, 0.8823604583740234, 0.9062730073928833], \n",
      "untrain_aligned_sim[:5]=[0.9882739186286926, 0.988780677318573, 0.9876015782356262, 0.9882155060768127, 0.9781569242477417], \n",
      "untrain_unaligned_sim[:5]=[0.8312917351722717, 0.8594483137130737, 0.8585234880447388, 0.8050487637519836, 0.8467270135879517]\n",
      "len(train_aligned_sim)=50000, len(train_unaligned_sim)=50000, len(untrain_aligned_sim)=50000, len(untrain_unaligned_sim)=50000\n"
     ]
    }
   ],
   "source": [
    "def get_cosine_similarities(msa_mapping, tokens, n_samples=50000, print_stats=False):\n",
    "\n",
    "    token_reps_trained, token_reps_untrained = tokens\n",
    "\n",
    "    aligned_pairs = helper.sample_aligned_pairs(\n",
    "    msa_mapping, len(msa_mapping), num_samples=n_samples\n",
    ")\n",
    "    unaligned_pairs = helper.sample_unaligned_pairs(aligned_pairs, msa_mapping)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate for Trained\n",
    "    print(\"Calculating similarities for TRAINED model...\")\n",
    "\n",
    "    train_aligned_sim = helper.get_cosine_similarity(\n",
    "    aligned_pairs, token_reps_trained, msa_mapping\n",
    ")\n",
    "    train_unaligned_sim = helper.get_cosine_similarity(\n",
    "    unaligned_pairs, token_reps_trained, msa_mapping\n",
    ")\n",
    "\n",
    "# Calculate for Untrained\n",
    "    print(\"Calculating similarities for UNTRAINED model...\")\n",
    "    untrain_aligned_sim = helper.get_cosine_similarity(\n",
    "    aligned_pairs, token_reps_untrained, msa_mapping\n",
    ")\n",
    "    untrain_unaligned_sim = helper.get_cosine_similarity(\n",
    "    unaligned_pairs, token_reps_untrained, msa_mapping\n",
    ")\n",
    "    if print_stats:\n",
    "        print(\n",
    "        f\"Done. Collected {len(train_aligned_sim)} aligned and {len(train_unaligned_sim)} unaligned pairs.\"\n",
    "    )\n",
    "\n",
    "        print(\n",
    "        f\"{train_aligned_sim[:5]=}, \\n{train_unaligned_sim[:5]=}, \\n{untrain_aligned_sim[:5]=}, \\n{untrain_unaligned_sim[:5]=}\"\n",
    "    )\n",
    "        print(\n",
    "        f\"{len(train_aligned_sim)=}, {len(train_unaligned_sim)=}, {len(untrain_aligned_sim)=}, {len(untrain_unaligned_sim)=}\"\n",
    "    )\n",
    "    return (\n",
    "        train_aligned_sim,\n",
    "        train_unaligned_sim,\n",
    "        untrain_aligned_sim,\n",
    "        untrain_unaligned_sim,\n",
    "    )\n",
    "\n",
    "train_aligned_sim,  train_unaligned_sim, untrain_aligned_sim, untrain_unaligned_sim = get_cosine_similarities(msa_mapping=msa_mapping, tokens=(token_reps_trained, token_reps_untrained), n_samples=50000, print_stats=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1170c",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ecd80448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score (Discrimination between Aligned/Unaligned):\n",
      "Trained Model:   0.8434\n",
      "Untrained Model: 0.7316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8434297752000001, 0.7316371854000001)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "\n",
    "# Plot Helper\n",
    "def plot_dist(ax, aligned, unaligned, title):\n",
    "    # Plot histograms with density\n",
    "    ax.hist(\n",
    "        aligned,\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        color=\"darkblue\",\n",
    "        label=\"Aligned pairs\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        unaligned,\n",
    "        bins=50,\n",
    "        density=True,\n",
    "        alpha=0.6,\n",
    "        color=\"lightblue\",\n",
    "        label=\"Unaligned pairs\",\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Cosine Similarity\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "\n",
    "# Plot Trained\n",
    "plot_dist(axes[0], train_aligned_sim, train_unaligned_sim, \"Transformer (Trained)\")\n",
    "\n",
    "# Plot Untrained\n",
    "plot_dist(\n",
    "    axes[1], untrain_aligned_sim, untrain_unaligned_sim, \"Transformer (Untrained)\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Evaluation (AUC) ---\n",
    "# Quantify the separation between the distributions\n",
    "def get_auc(train_aligned_sim, train_unaligned_sim, untrain_aligned_sim, untrain_unaligned_sim, print_stats=False):\n",
    "    def calculate_auc(aligned, unaligned):\n",
    "        y_true = [1] * len(aligned) + [0] * len(unaligned)\n",
    "        y_scores = np.concatenate([aligned, unaligned])\n",
    "        return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "\n",
    "    auc_trained = calculate_auc(train_aligned_sim, train_unaligned_sim)\n",
    "    auc_untrained = calculate_auc(untrain_aligned_sim, untrain_unaligned_sim)\n",
    "\n",
    "    if print_stats:\n",
    "        print(f\"AUC Score (Discrimination between Aligned/Unaligned):\")\n",
    "        print(f\"Trained Model:   {auc_trained:.4f}\")\n",
    "        print(f\"Untrained Model: {auc_untrained:.4f}\")\n",
    "    return auc_trained, auc_untrained\n",
    "get_auc(\n",
    "    train_aligned_sim,\n",
    "    train_unaligned_sim,\n",
    "    untrain_aligned_sim,\n",
    "    untrain_unaligned_sim,\n",
    "    print_stats=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ab9bf29-bf10-463a-9f44-4ed61cb32641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "if \"families\" not in locals() and \"families\" not in globals():\n",
    "    with gzip.open(\"data/pfam_filtered_families.pkl.gz\", \"rb\") as f:\n",
    "        families = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "76166a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<<class 'Bio.Align.MultipleSeqAlignment'> instance (1092980 records of length 2135) at 11bdd3260>,\n",
       " <<class 'Bio.Align.MultipleSeqAlignment'> instance (289903 records of length 3373) at 3532014c0>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "families = families[:2]\n",
    "families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2ab8a545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1977\n",
      "2135\n"
     ]
    }
   ],
   "source": [
    "print(families[0][0].seq.count('-'))\n",
    "print(len(families[0][0].seq ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a8638780-1294-41b9-938c-c8098de8e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 128 sequences...\n",
      "Running forward pass on TRAINED model...\n",
      "Processing 128 sequences in batches of 1...\n",
      "Running forward pass on UNTRAINED model...\n",
      "Processing 128 sequences in batches of 1...\n",
      "Calculating similarities for TRAINED model...\n",
      "Calculating similarities for UNTRAINED model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:15<01:15, 75.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 128 sequences...\n",
      "Running forward pass on TRAINED model...\n",
      "Processing 128 sequences in batches of 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:44<01:44, 104.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m msa = get_128_seq(family)\n\u001b[32m     18\u001b[39m clean_seqs, msa_mapping = clean_and_map_msa(msa)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m token_reps_trained, token_reps_untrained = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_seqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m train_aligned_sim,  train_unaligned_sim, untrain_aligned_sim, untrain_unaligned_sim = get_cosine_similarities(msa_mapping=msa_mapping, tokens=(token_reps_trained, token_reps_untrained), n_samples=\u001b[32m50000\u001b[39m, print_stats=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     23\u001b[39m trained_auc, untrained_auc = get_auc(\n\u001b[32m     24\u001b[39m     train_aligned_sim,\n\u001b[32m     25\u001b[39m     train_unaligned_sim,\n\u001b[32m     26\u001b[39m     untrain_aligned_sim,\n\u001b[32m     27\u001b[39m     untrain_unaligned_sim,\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(clean_seqs)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Note: We use the helper, but we pass the labels=None explicitly if your helper expects it\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         token_reps_trained, batch_strs_trained = \u001b[43mhelper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_hidden_representations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphabet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_seqs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_seqs\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# B. Untrained Model (Random Baseline)\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning forward pass on UNTRAINED model...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/helpers/helper.py:77\u001b[39m, in \u001b[36mget_hidden_representations\u001b[39m\u001b[34m(model, alphabet, labels, sequences, batch_size)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# 4. Inferenz (Memory Management ist hier wichtig)\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrepr_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Ergebnis extrahieren (Shape: [Batch_Size, Max_Seq_Len_in_Batch, Hidden_Dim])\u001b[39;00m\n\u001b[32m     82\u001b[39m token_representations = results[\u001b[33m\"\u001b[39m\u001b[33mrepresentations\u001b[39m\u001b[33m\"\u001b[39m][repr_layer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/esm/model/esm1.py:156\u001b[39m, in \u001b[36mProteinBertModel.forward\u001b[39m\u001b[34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[39m\n\u001b[32m    153\u001b[39m     padding_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     x, attn = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_head_weights\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx + \u001b[32m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[32m    160\u001b[39m         hidden_representations[layer_idx + \u001b[32m1\u001b[39m] = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/esm/modules.py:125\u001b[39m, in \u001b[36mTransformerLayer.forward\u001b[39m\u001b[34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[39m\n\u001b[32m    123\u001b[39m residual = x\n\u001b[32m    124\u001b[39m x = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(x)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m x, attn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m x = residual + x\n\u001b[32m    136\u001b[39m residual = x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/esm/multihead_attention.py:208\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    197\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rot_emb\n\u001b[32m    198\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enable_torch_version\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_head_weights\n\u001b[32m    206\u001b[39m ):\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m incremental_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    232\u001b[39m     saved_state = \u001b[38;5;28mself\u001b[39m._get_input_buffer(incremental_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/functional.py:6322\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6320\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6321\u001b[39m         b_q, b_k, b_v = in_proj_bias.chunk(\u001b[32m3\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m6322\u001b[39m     q, k, v = \u001b[43m_in_projection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6325\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6326\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   6332\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6334\u001b[39m \u001b[38;5;66;03m# prep attention mask\u001b[39;00m\n\u001b[32m   6336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6337\u001b[39m     \u001b[38;5;66;03m# ensure attn_mask's dim is 3\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5802\u001b[39m, in \u001b[36m_in_projection\u001b[39m\u001b[34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[39m\n\u001b[32m   5796\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m b_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b_k.shape == (Eq,), (\n\u001b[32m   5797\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpecting key bias shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_k.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5798\u001b[39m )\n\u001b[32m   5799\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m b_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b_v.shape == (Eq,), (\n\u001b[32m   5800\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpecting value bias shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_v.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   5801\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m5802\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m linear(q, w_q, b_q), linear(k, w_k, b_k), \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_v\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "%autoreload\n",
    "\n",
    "trained_results = []\n",
    "untrained_results = []\n",
    "\n",
    "def get_128_seq(msa):\n",
    "    msa = list(msa)\n",
    "    n_samples = min(128, len(msa))\n",
    "    return random.sample(msa, n_samples)   \n",
    "\n",
    "for family in tqdm(families):\n",
    "\n",
    "    msa = get_128_seq(family)\n",
    "\n",
    "    clean_seqs, msa_mapping = clean_and_map_msa(msa)\n",
    "    token_reps_trained, token_reps_untrained = forward_pass(clean_seqs)\n",
    "\n",
    "    train_aligned_sim,  train_unaligned_sim, untrain_aligned_sim, untrain_unaligned_sim = get_cosine_similarities(msa_mapping=msa_mapping, tokens=(token_reps_trained, token_reps_untrained), n_samples=50000, print_stats=False)\n",
    "    \n",
    "    trained_auc, untrained_auc = get_auc(\n",
    "        train_aligned_sim,\n",
    "        train_unaligned_sim,\n",
    "        untrain_aligned_sim,\n",
    "        untrain_unaligned_sim,\n",
    "    )\n",
    "    trained_results.append(trained_auc)\n",
    "    untrained_results.append(untrained_auc)\n",
    "\n",
    "print(\"Experiments complete.\")\n",
    "print(f\"average Trained AUCs: {np.mean(np.array(trained_results)) }\")\n",
    "print(f\"average Untrained AUCs: {np.mean(np.array(untrained_results)) }\")\n",
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdd2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised-learning-in-biology (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
