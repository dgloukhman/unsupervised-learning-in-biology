{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e6394d",
   "metadata": {},
   "source": [
    "# Learning Encodes Alignment within a Protein Family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e91d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to import modules from helpers\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Gehe einen Ordner nach oben\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f5a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "from helpers import helper\n",
    "import time\n",
    "import io\n",
    "import urllib3\n",
    "import requests\n",
    "\n",
    "# kleineres Modell 'esm2_t6_8M_UR50D' zum testen \n",
    "# verwendet 36-layer Transformer trained on UniParc\" (ca. 670 Mio. Parameter ) im Paper.\n",
    "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Modell auf GPU geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2009d",
   "metadata": {},
   "source": [
    "### Hypothesis 1:\n",
    "final hidden representations of a sequence encode information about the family it belongs to.\n",
    "\n",
    "### Method:\n",
    "\n",
    "- Get Dataset (Pfam)\n",
    "- compare the distribution of cosine similarities of representations between pairs of residues that are aligned in the family’s MSA background distribution of cosine similarities between unaligned pairs of residues.\n",
    "- Compare with distributions befor learning (We need the embeddings befor pretraining (randomize model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6797d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Pfam database to find PF01010...\n",
      "This may take 1-2 minutes as it searches the compressed stream.\n",
      "\n",
      "Success! Loaded Q8HUX0_9ROSA/106-348\n",
      "Sequence count: 76\n",
      "Alignment length: 344\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import urllib.request\n",
    "from Bio import AlignIO\n",
    "from io import StringIO\n",
    "\n",
    "def get_pfam_seed_by_id(pfam_accession):\n",
    "    \"\"\"\n",
    "    Streams the Pfam-A seed database from EBI FTP and extracts a specific family.\n",
    "    Bypasses broken API endpoints.\n",
    "    \"\"\"\n",
    "    # Official EBI FTP URL for the current release of Pfam Seed Alignments\n",
    "    url = \"http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.seed.gz\"\n",
    "    \n",
    "    print(f\"Streaming Pfam database to find {pfam_accession}...\")\n",
    "    print(\"This may take 1-2 minutes as it searches the compressed stream.\")\n",
    "\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        # Decompress the stream on the fly\n",
    "        with gzip.open(response, 'rt') as f:\n",
    "            entry_buffer = []\n",
    "            for line in f:\n",
    "                entry_buffer.append(line)\n",
    "                # End of a record in Stockholm format\n",
    "                if line.startswith(\"//\"):\n",
    "                    block = \"\".join(entry_buffer)\n",
    "                    # Check if this block matches our target Accession\n",
    "                    if f\"AC   {pfam_accession}\" in block:\n",
    "                        return block\n",
    "                    entry_buffer = [] # Reset buffer for next entry\n",
    "    return None\n",
    "\n",
    "# 1. Fetch the data for PF01010 (Response regulator receiver domain)\n",
    "msa_data = get_pfam_seed_by_id(\"PF01010\")\n",
    "\n",
    "if msa_data:\n",
    "    # 2. Parse using BioPython\n",
    "    msa = AlignIO.read(StringIO(msa_data), \"stockholm\")\n",
    "    \n",
    "    # 3. Verify\n",
    "    print(f\"\\nSuccess! Loaded {msa[0].id}\")\n",
    "    print(f\"Sequence count: {len(msa)}\")\n",
    "    print(f\"Alignment length: {msa.get_alignment_length()}\")\n",
    "else:\n",
    "    print(\"Family not found in the current Pfam release.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e52f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne TRAINIERTE Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# 1. Embeddings mit dem TRAINIERTEN Modell holen\n",
    "print(\"Berechne TRAINIERTE Embeddings...\")\n",
    "# Schritt 1: Hidden Representations holen\n",
    "token_reps_trained, batch_strs_trained = helper.get_hidden_representations(model, alphabet, labels, seqs)\n",
    "# Schritt 2: Mean Pooling durchführen\n",
    "emb_trained = helper.get_protein_embedding(token_reps_trained, batch_strs_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne UNTRAINIERTE Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# 2. Embeddings mit einem zufälligen (UNTRAINIERTEN) Modell holen (natürlich ist hier ein Problem, \n",
    "# dass wir den seed nicht kennen alleine deshalb werden sich hier Sachen vom original Paper unterscheiden)\n",
    "print(\"Berechne UNTRAINIERTE Embeddings...\")\n",
    "untrained_model = helper.randomize_model(model)\n",
    "if torch.cuda.is_available(): \n",
    "    untrained_model = untrained_model.cuda()\n",
    "\n",
    "# Schritt 1: Hidden Representations holen (mit untrainiertem Modell)\n",
    "token_reps_untrained, batch_strs_untrained = helper.get_hidden_representations(untrained_model, alphabet, labels, seqs)\n",
    "# Schritt 2: Mean Pooling durchführen\n",
    "emb_untrained = helper.get_protein_embedding(token_reps_untrained, batch_strs_untrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised-learning-in-biology (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
