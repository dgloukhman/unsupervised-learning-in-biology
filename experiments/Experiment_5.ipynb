{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e6394d",
   "metadata": {},
   "source": [
    "# Linear Model for structure prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84674467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to import modules from helpers\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import gzip\n",
    "import urllib.request # TODO: Outsource data download to helper script\n",
    "\n",
    "import esm\n",
    "from io import StringIO\n",
    "from Bio import SeqIO\n",
    "from Bio import AlignIO\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Gehe einen Ordner nach oben\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from helpers import helper\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f45c27",
   "metadata": {},
   "source": [
    "# Load ESM model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d759a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kleineres Modell 'esm2_t6_8M_UR50D' zum testen \n",
    "# verwendet 36-layer Transformer trained on UniParc\" (ca. 670 Mio. Parameter ) im Paper.\n",
    "model, alphabet = esm.pretrained.esm1_t6_43M_UR50S()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"Modell auf GPU geladen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8225e55",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2835e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n",
      "Parsing and filtering sequences...\n",
      "Total sequences retained: 12261\n",
      "  domain_id                                           sequence class fold  \\\n",
      "0   d1dlwa_  slfeqlggqaavqavtaqfyaniqadatvatffngidmpnqtnkta...     a  a.1   \n",
      "1   d2gkma_  gllsrlrkrepisiydkiggheaievvvedffvrvladdqlsaffs...     a  a.1   \n",
      "2   d1ngka_  ksfydavggaktfdaivsrfyaqvaedevlrrvypeddlagaeerl...     a  a.1   \n",
      "3   d2bkma_  eqwqtlyeaiggeetvaklveafyrrvaahpdlrpifpddltetah...     a  a.1   \n",
      "4   d4i0va_  aslyeklggaaavdlavekfygkvladervnrffvntdmakqkqhq...     a  a.1   \n",
      "\n",
      "  superfamily   family  \n",
      "0       a.1.1  a.1.1.1  \n",
      "1       a.1.1  a.1.1.1  \n",
      "2       a.1.1  a.1.1.1  \n",
      "3       a.1.1  a.1.1.1  \n",
      "4       a.1.1  a.1.1.1  \n"
     ]
    }
   ],
   "source": [
    "# Download, Parse and Filter SCOP data\n",
    "helper.download_no_requests()\n",
    "df_scop = helper.parse_and_filter_scop()\n",
    "\n",
    "if not df_scop.empty:\n",
    "    print(f\"Total sequences retained: {len(df_scop)}\")\n",
    "    print(df_scop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649fdd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitionierung abgeschlossen.\n",
      "Verfügbare Split-Level: ['family', 'superfamily', 'fold']\n",
      "Anzahl Folds pro Level: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Wir erstellen ein Dictionary, das für jedes Level ('family', 'superfamily', 'fold')\n",
    "# die entsprechenden 5-Fold Indizes speichert.\n",
    "levels = ['family', 'superfamily', 'fold']\n",
    "partitions = {}\n",
    "\n",
    "for level in levels:\n",
    "    # Initialisiere GroupKFold mit 5 Splits\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    # Die \"Gruppen\" sind die Labels in der jeweiligen Spalte (z.B. 'a.1.1.1' für family)\n",
    "    groups = df_scop[level].values\n",
    "    \n",
    "    # Erstelle die Liste der (train_index, test_index) Tupel\n",
    "    # list() materialisiert den Generator, damit wir die Indizes später wiederverwenden können\n",
    "    partitions[level] = list(gkf.split(df_scop, groups=groups))\n",
    "\n",
    "print(\"Partitionierung abgeschlossen.\")\n",
    "print(f\"Verfügbare Split-Level: {list(partitions.keys())}\")\n",
    "print(f\"Anzahl Folds pro Level: {len(partitions['fold'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dca6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# partitions['family'][4] 5. Fold für 'family' Level\n",
    "# partitions['family'][4][0]  -> train indices\n",
    "# partitions['family'][4][1]  -> test indices\n",
    "# These here are the non intersecting train and test indices for the 5th fold of 'family' level\n",
    "# print(partitions['family'][4][0])\n",
    "# print(partitions['family'][4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9395c02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>class</th>\n",
       "      <th>fold</th>\n",
       "      <th>superfamily</th>\n",
       "      <th>family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>d1asha_</td>\n",
       "      <td>anktrelcmkslehakvdtsnearqdgidlykhmfenypplrkyfk...</td>\n",
       "      <td>a</td>\n",
       "      <td>a.1</td>\n",
       "      <td>a.1.1</td>\n",
       "      <td>a.1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d2dc3a_</td>\n",
       "      <td>eelseaerkavqamwarlyancedvgvailvrffvnfpsakqyfsq...</td>\n",
       "      <td>a</td>\n",
       "      <td>a.1</td>\n",
       "      <td>a.1.1</td>\n",
       "      <td>a.1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d4hswa_</td>\n",
       "      <td>gfkqdiatirgdlrtyaqdiflaflnkypderryfknyvgksdqel...</td>\n",
       "      <td>a</td>\n",
       "      <td>a.1</td>\n",
       "      <td>a.1.1</td>\n",
       "      <td>a.1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d1ecaa_</td>\n",
       "      <td>lsadqistvqasfdkvkgdpvgilyavfkadpsimakftqfagkdl...</td>\n",
       "      <td>a</td>\n",
       "      <td>a.1</td>\n",
       "      <td>a.1.1</td>\n",
       "      <td>a.1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>d1x9fd_</td>\n",
       "      <td>eclvteslkvklqwasafghahervafglelwrdiiddhpeikapf...</td>\n",
       "      <td>a</td>\n",
       "      <td>a.1</td>\n",
       "      <td>a.1.1</td>\n",
       "      <td>a.1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>d2ggva_</td>\n",
       "      <td>tdmwiertadiswesdaeitgsservdvrldddgnfqlmndpga</td>\n",
       "      <td>g</td>\n",
       "      <td>g.96</td>\n",
       "      <td>g.96.1</td>\n",
       "      <td>g.96.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12256</th>\n",
       "      <td>d3s2ra_</td>\n",
       "      <td>ginpeirknedkvvdsvvvtelsknitpycrcwrsgtfplcdgscv...</td>\n",
       "      <td>g</td>\n",
       "      <td>g.97</td>\n",
       "      <td>g.97.1</td>\n",
       "      <td>g.97.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12257</th>\n",
       "      <td>d4c3hd_</td>\n",
       "      <td>sattlntpvvihatqlpqhvstdevlqflesfidekeniidsttmn...</td>\n",
       "      <td>g</td>\n",
       "      <td>g.98</td>\n",
       "      <td>g.98.1</td>\n",
       "      <td>g.98.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12258</th>\n",
       "      <td>d3jb9e_</td>\n",
       "      <td>rlrtsrtkrppdgfdeieptliefqdrmrqientmgkgtktemlap...</td>\n",
       "      <td>g</td>\n",
       "      <td>g.99</td>\n",
       "      <td>g.99.1</td>\n",
       "      <td>g.99.1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12260</th>\n",
       "      <td>d5syba_</td>\n",
       "      <td>akhhpdlifcrkqagvaigrlcekcdgkcvicdsyvrpstlvricd...</td>\n",
       "      <td>g</td>\n",
       "      <td>g.101</td>\n",
       "      <td>g.101.1</td>\n",
       "      <td>g.101.1.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9809 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      domain_id                                           sequence class  \\\n",
       "5       d1asha_  anktrelcmkslehakvdtsnearqdgidlykhmfenypplrkyfk...     a   \n",
       "6       d2dc3a_  eelseaerkavqamwarlyancedvgvailvrffvnfpsakqyfsq...     a   \n",
       "7       d4hswa_  gfkqdiatirgdlrtyaqdiflaflnkypderryfknyvgksdqel...     a   \n",
       "8       d1ecaa_  lsadqistvqasfdkvkgdpvgilyavfkadpsimakftqfagkdl...     a   \n",
       "9       d1x9fd_  eclvteslkvklqwasafghahervafglelwrdiiddhpeikapf...     a   \n",
       "...         ...                                                ...   ...   \n",
       "12255   d2ggva_       tdmwiertadiswesdaeitgsservdvrldddgnfqlmndpga     g   \n",
       "12256   d3s2ra_  ginpeirknedkvvdsvvvtelsknitpycrcwrsgtfplcdgscv...     g   \n",
       "12257   d4c3hd_  sattlntpvvihatqlpqhvstdevlqflesfidekeniidsttmn...     g   \n",
       "12258   d3jb9e_  rlrtsrtkrppdgfdeieptliefqdrmrqientmgkgtktemlap...     g   \n",
       "12260   d5syba_  akhhpdlifcrkqagvaigrlcekcdgkcvicdsyvrpstlvricd...     g   \n",
       "\n",
       "        fold superfamily     family  \n",
       "5        a.1       a.1.1    a.1.1.2  \n",
       "6        a.1       a.1.1    a.1.1.2  \n",
       "7        a.1       a.1.1    a.1.1.2  \n",
       "8        a.1       a.1.1    a.1.1.2  \n",
       "9        a.1       a.1.1    a.1.1.2  \n",
       "...      ...         ...        ...  \n",
       "12255   g.96      g.96.1   g.96.1.1  \n",
       "12256   g.97      g.97.1   g.97.1.0  \n",
       "12257   g.98      g.98.1   g.98.1.1  \n",
       "12258   g.99      g.99.1   g.99.1.1  \n",
       "12260  g.101     g.101.1  g.101.1.1  \n",
       "\n",
       "[9809 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scop.iloc[partitions['family'][4][0]] # Train set for 5th fold of 'family' level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfca1975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bearbeite d1dlwa_ -> PDB: 1dlw, Chain: A\n",
      "Downloading PDB structure '1dlw'...\n",
      "Fehler bei 1dlw: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2gkma_ -> PDB: 2gkm, Chain: A\n",
      "Downloading PDB structure '2gkm'...\n",
      "Fehler bei 2gkm: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1ngka_ -> PDB: 1ngk, Chain: A\n",
      "Downloading PDB structure '1ngk'...\n",
      "Fehler bei 1ngk: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2bkma_ -> PDB: 2bkm, Chain: A\n",
      "Downloading PDB structure '2bkm'...\n",
      "Fehler bei 2bkm: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d4i0va_ -> PDB: 4i0v, Chain: A\n",
      "Downloading PDB structure '4i0v'...\n",
      "Desired structure not found or download failed. '4i0v': HTTP Error 403: Forbidden\n",
      "Fehler bei 4i0v: 'NoneType' object has no attribute 'readlines'\n",
      "Bearbeite d1asha_ -> PDB: 1ash, Chain: A\n",
      "Downloading PDB structure '1ash'...\n",
      "Fehler bei 1ash: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2dc3a_ -> PDB: 2dc3, Chain: A\n",
      "Downloading PDB structure '2dc3'...\n",
      "Fehler bei 2dc3: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d4hswa_ -> PDB: 4hsw, Chain: A\n",
      "Downloading PDB structure '4hsw'...\n",
      "Fehler bei 4hsw: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1ecaa_ -> PDB: 1eca, Chain: A\n",
      "Downloading PDB structure '1eca'...\n",
      "Fehler bei 1eca: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1x9fd_ -> PDB: 1x9f, Chain: D\n",
      "Downloading PDB structure '1x9f'...\n",
      "Fehler bei 1x9f: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1x9fc_ -> PDB: 1x9f, Chain: C\n",
      "Structure exists: './pdb_files/pdb1x9f.ent' \n",
      "Fehler bei 1x9f: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1jl7a_ -> PDB: 1jl7, Chain: A\n",
      "Downloading PDB structure '1jl7'...\n",
      "Fehler bei 1jl7: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1it2a_ -> PDB: 1it2, Chain: A\n",
      "Downloading PDB structure '1it2'...\n",
      "Fehler bei 1it2: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1or4a_ -> PDB: 1or4, Chain: A\n",
      "Downloading PDB structure '1or4'...\n",
      "Fehler bei 1or4: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1itha_ -> PDB: 1ith, Chain: A\n",
      "Downloading PDB structure '1ith'...\n",
      "Fehler bei 1ith: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3g46a_ -> PDB: 3g46, Chain: A\n",
      "Downloading PDB structure '3g46'...\n",
      "Fehler bei 3g46: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1b0ba_ -> PDB: 1b0b, Chain: A\n",
      "Downloading PDB structure '1b0b'...\n",
      "Fehler bei 1b0b: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1gcva_ -> PDB: 1gcv, Chain: A\n",
      "Downloading PDB structure '1gcv'...\n",
      "Fehler bei 1gcv: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3d1kb_ -> PDB: 3d1k, Chain: B\n",
      "Downloading PDB structure '3d1k'...\n",
      "Fehler bei 3d1k: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1cg5b_ -> PDB: 1cg5, Chain: B\n",
      "Downloading PDB structure '1cg5'...\n",
      "Fehler bei 1cg5: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1gcvb_ -> PDB: 1gcv, Chain: B\n",
      "Structure exists: './pdb_files/pdb1gcv.ent' \n",
      "Fehler bei 1gcv: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1hlba_ -> PDB: 1hlb, Chain: A\n",
      "Downloading PDB structure '1hlb'...\n",
      "Fehler bei 1hlb: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1tu9a1 -> PDB: 1tu9, Chain: A\n",
      "Downloading PDB structure '1tu9'...\n",
      "Fehler bei 1tu9: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1mbaa_ -> PDB: 1mba, Chain: A\n",
      "Downloading PDB structure '1mba'...\n",
      "Fehler bei 1mba: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1naza_ -> PDB: 1naz, Chain: A\n",
      "Downloading PDB structure '1naz'...\n",
      "Fehler bei 1naz: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1q1fa_ -> PDB: 1q1f, Chain: A\n",
      "Downloading PDB structure '1q1f'...\n",
      "Fehler bei 1q1f: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1h97a_ -> PDB: 1h97, Chain: A\n",
      "Downloading PDB structure '1h97'...\n",
      "Fehler bei 1h97: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2w72a_ -> PDB: 2w72, Chain: A\n",
      "Downloading PDB structure '2w72'...\n",
      "Fehler bei 2w72: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3qm9a_ -> PDB: 3qm9, Chain: A\n",
      "Downloading PDB structure '3qm9'...\n",
      "Fehler bei 3qm9: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3qqqa_ -> PDB: 3qqq, Chain: A\n",
      "Downloading PDB structure '3qqq'...\n",
      "Fehler bei 3qqq: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3l0fa_ -> PDB: 3l0f, Chain: A\n",
      "Downloading PDB structure '3l0f'...\n",
      "Fehler bei 3l0f: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1b8da_ -> PDB: 1b8d, Chain: A\n",
      "Downloading PDB structure '1b8d'...\n",
      "Fehler bei 1b8d: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d1xg0c_ -> PDB: 1xg0, Chain: C\n",
      "Downloading PDB structure '1xg0'...\n",
      "Fehler bei 1xg0: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d4po5b_ -> PDB: 4po5, Chain: B\n",
      "Downloading PDB structure '4po5'...\n",
      "Fehler bei 4po5: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2xkia_ -> PDB: 2xki, Chain: A\n",
      "Downloading PDB structure '2xki'...\n",
      "Fehler bei 2xki: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2wy4a_ -> PDB: 2wy4, Chain: A\n",
      "Downloading PDB structure '2wy4'...\n",
      "Fehler bei 2wy4: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d2ig3a_ -> PDB: 2ig3, Chain: A\n",
      "Downloading PDB structure '2ig3'...\n",
      "Fehler bei 2ig3: File type must be PDB, mmCIF or DSSP\n",
      "Bearbeite d3pt8a_ -> PDB: 3pt8, Chain: A\n",
      "Downloading PDB structure '3pt8'...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     70\u001b[39m pdb_id, chain_id = parse_scop_id(dom_id)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBearbeite \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdom_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> PDB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Chain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m ss_string = \u001b[43mget_dssp_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdb_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ss_string:\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# WICHTIG: Die Länge des DSSP-Strings muss mit der Sequenz übereinstimmen!\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# PDB-Dateien haben oft fehlende Residues. Du musst eventuell alignen.\u001b[39;00m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# Für den Anfang speichern wir einfach das Ergebnis.\u001b[39;00m\n\u001b[32m     80\u001b[39m     df_scop.at[index, \u001b[33m'\u001b[39m\u001b[33mss8_label\u001b[39m\u001b[33m'\u001b[39m] = ss_string\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_dssp_labels\u001b[39m\u001b[34m(pdb_id, chain_id, pdb_dir)\u001b[39m\n\u001b[32m     11\u001b[39m parser = PDBParser(QUIET=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# PDB-Datei herunterladen (oder lokalen Pfad nutzen)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# SCOP IDs wie 'd1asha_' basieren auf PDB ID '1ash'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m ent_filename = \u001b[43mpdbl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve_pdb_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdb_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpdb_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpdb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     18\u001b[39m     structure = parser.get_structure(pdb_id, ent_filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/unsupervised-learning-in-biology/.venv/lib/python3.12/site-packages/Bio/PDB/PDBList.py:338\u001b[39m, in \u001b[36mPDBList.retrieve_pdb_file\u001b[39m\u001b[34m(self, pdb_code, obsolete, pdir, file_format, overwrite)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    337\u001b[39m     urlcleanup()\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDesired structure not found or download failed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpdb_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/urllib/request.py:268\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[32m    266\u001b[39m     reporthook(blocknum, bs, size)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m block := \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    269\u001b[39m     read += \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[32m    270\u001b[39m     tfp.write(block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio.PDB import PDBList, PDBParser, DSSP\n",
    "import os\n",
    "\n",
    "# 1. Hilfsfunktion zum Laden der Struktur und Extrahieren von DSSP\n",
    "def get_dssp_labels(pdb_id, chain_id, pdb_dir=\"./pdb_files\"):\n",
    "    \"\"\"\n",
    "    Lädt PDB (falls nötig), berechnet DSSP und gibt den 8-Class-String zurück.\n",
    "    \"\"\"\n",
    "    pdbl = PDBList()\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    \n",
    "    # PDB-Datei herunterladen (oder lokalen Pfad nutzen)\n",
    "    # SCOP IDs wie 'd1asha_' basieren auf PDB ID '1ash'\n",
    "    ent_filename = pdbl.retrieve_pdb_file(pdb_id, pdir=pdb_dir, file_format=\"pdb\")\n",
    "    \n",
    "    try:\n",
    "        structure = parser.get_structure(pdb_id, ent_filename)\n",
    "        model = structure[0]\n",
    "        \n",
    "        # DSSP berechnen\n",
    "        # Hinweis: Stelle sicher, dass 'mkdssp' oder 'dssp' im System-Pfad ist\n",
    "        dssp = DSSP(model, ent_filename)\n",
    "        \n",
    "        # DSSP Keys sind Tupel (Chain, ResID). Wir filtern nach unserer Chain.\n",
    "        # Die Eigenschaft für Sekundärstruktur ist Index 2 in der DSSP-Liste\n",
    "        sec_structure = []\n",
    "        \n",
    "        for key in dssp.keys():\n",
    "            if key[0] == chain_id:\n",
    "                ss_code = dssp[key][2]\n",
    "                # Leere Rückgaben oder '-' werden oft als Coil ('C') gewertet\n",
    "                if ss_code == '-' or ss_code == ' ':\n",
    "                    ss_code = 'C' \n",
    "                sec_structure.append(ss_code)\n",
    "                \n",
    "        return \"\".join(sec_structure)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei {pdb_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# 2. Anwendung auf deinen DataFrame\n",
    "# Deine Domain ID 'd1asha_' ist im SCOP-Format.\n",
    "# Konvention: d + PDB_ID (4 chars) + Chain (1 char) + _ (optional)\n",
    "# Beispiel: d1asha_ -> PDB: 1ash, Chain: A\n",
    "\n",
    "def parse_scop_id(domain_id):\n",
    "    # Einfache Heuristik für Standard SCOP IDs\n",
    "    # d1asha_ -> 1ash, A\n",
    "    clean_id = domain_id[1:] # Entferne 'd'\n",
    "    pdb_id = clean_id[:4]\n",
    "    chain_id = clean_id[4]\n",
    "    if chain_id == '_': \n",
    "        # Manchmal bedeutet _ \"keine Chain\" oder \"Chain A\", \n",
    "        # in SCOP oft Chain A oder die einzige vorhandene.\n",
    "        # Hier muss man eventuell aufpassen.\n",
    "        chain_id = 'A' \n",
    "    return pdb_id, chain_id.upper()\n",
    "\n",
    "# Beispiel-Loop (Vorsicht: Das Herunterladen dauert!)\n",
    "# Erstelle erst den Ordner für PDBs\n",
    "os.makedirs(\"./pdb_files\", exist_ok=True)\n",
    "\n",
    "# Neue Spalte erstellen\n",
    "df_scop['ss8_label'] = \"\"\n",
    "\n",
    "for index, row in df_scop.iterrows():\n",
    "    dom_id = row['domain_id']\n",
    "    pdb_id, chain_id = parse_scop_id(dom_id)\n",
    "    \n",
    "    print(f\"Bearbeite {dom_id} -> PDB: {pdb_id}, Chain: {chain_id}\")\n",
    "    \n",
    "    ss_string = get_dssp_labels(pdb_id, chain_id)\n",
    "    \n",
    "    if ss_string:\n",
    "        # WICHTIG: Die Länge des DSSP-Strings muss mit der Sequenz übereinstimmen!\n",
    "        # PDB-Dateien haben oft fehlende Residues. Du musst eventuell alignen.\n",
    "        # Für den Anfang speichern wir einfach das Ergebnis.\n",
    "        df_scop.at[index, 'ss8_label'] = ss_string\n",
    "\n",
    "print(df_scop.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ff126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the domains_id's and the actual domain sequences from the DataFrame\n",
    "labels = df_scop['domain_id'].tolist()\n",
    "seqs = [s.upper() for s in df_scop['sequence'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berechne TRAINIERTE Embeddings für 100 Sequenzen...\n",
      "Processing 100 sequences in batches of 1...\n",
      "Fertig!\n",
      "Erstellte Embeddings: 100\n"
     ]
    }
   ],
   "source": [
    "# 1. Embeddings mit dem TRAINIERTEN Modell holen\n",
    "print(f\"Berechne TRAINIERTE Embeddings für {len(seqs)} Sequenzen...\")\n",
    "\n",
    "# Schritt 1: Hidden Representations holen\n",
    "token_reps_trained, batch_strs_trained = helper.get_hidden_representations(model, alphabet, labels, seqs)\n",
    "\n",
    "# Schritt 2: Mean Pooling durchführen\n",
    "emb_trained = helper.get_protein_embedding(token_reps_trained, batch_strs_trained)\n",
    "\n",
    "print(\"Fertig!\")\n",
    "print(f\"Erstellte Embeddings: {len(emb_trained)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating UNTRAINED Embeddings...\n",
      "Processing 200 sequences in batches of 1...\n"
     ]
    }
   ],
   "source": [
    "# 2. Get embeddings before pretraining (natürlich ist hier ein Problem, dass wir den seed nicht kennen alleine deshalb werden sich hier Sachen vom original Paper unterscheiden)\n",
    "print(\"Calculating UNTRAINED Embeddings...\")\n",
    "untrained_model = helper.randomize_model(model)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    untrained_model = untrained_model.cuda()\n",
    "\n",
    "# Schritt 1: Calculate final hidden representations\n",
    "token_reps_untrained, batch_strs_untrained = helper.get_hidden_representations(untrained_model, alphabet, labels, seqs)\n",
    "\n",
    "# Schritt 2: Calculate Untrained Embeddings\n",
    "emb_untrained = helper.get_protein_embedding(token_reps_untrained, batch_strs_untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2009d",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "One of the oldest assumptions of sequencing biology: The underlying structure of a protein is a hidden variable that influences the patterns observed in sequence data. And vice versa the patterns observed in the sequence data influence the dtructure of a protein.\n",
    "In short: Structural information is encoded in the sequences.\n",
    "\n",
    "- secondary structure decides local choice and order of sequences\n",
    "- tertiary decides over long range choice and order of sequences\n",
    "\n",
    "Underlying general Hypothesis: Since 3d struture is encoded in the sequences. It is a logical hypothesis that via unsupervised learning the model learns to decode the hidden information about the secondary and tertiary strucure of the protein implicitly. \n",
    "\n",
    "In the paper they start by using simple linear models on top of the learned respresentations to see whether or not even simple models can infer about structure using the learned representations. If they are able to do that that would be very impressive.\n",
    "\n",
    "enabling a direct inspection of the structural content of representations.\n",
    "\n",
    "By comparing representations of the Transformer before and after pretraining, we can identify the information that emerges as a result of the unsupervised learning\n",
    "\n",
    "fivefold cross validation experiment to study generalization of structural information at the family, superfamily, and fold level.\n",
    "-\tFor each of the three levels, we construct a dataset of 15,297 protein structures using the SCOPe database.\n",
    "\n",
    "### Hypothesis 1:\n",
    "\n",
    "\n",
    "### Hypothesis 2:\n",
    "final hidden representations of a sequence encode information about the family it belongs to.\n",
    "\n",
    "### Method:\n",
    "\n",
    "- Get Dataset (Pfam)\n",
    "- compare the distribution of cosine similarities of representations between pairs of residues that are aligned in the family’s MSA background distribution of cosine similarities between unaligned pairs of residues.\n",
    "- Compare with distributions befor learning (We need the embeddings befor pretraining (randomize model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1. Bereinigung: Filterung von Sequenzen, bei denen Länge(Seq) != Länge(DSSP)\n",
    "# und Mapping der 8 Klassen auf Integers.\n",
    "\n",
    "valid_indices = []\n",
    "ss8_mapping = {\n",
    "    'H': 0, 'B': 1, 'E': 2, 'G': 3, 'I': 4, 'T': 5, 'S': 6, \n",
    "    'C': 7, '-': 7, ' ': 7\n",
    "}\n",
    "\n",
    "# Liste für die bereinigten Daten\n",
    "cleaned_data = []\n",
    "\n",
    "# Wir gehen davon aus, dass token_reps_trained in der gleichen Reihenfolge vorliegt wie df_scop\n",
    "# Hinweis: Das erste Token (Start-Token) und letzte Token (End-Token) vom ESM-Modell müssen beachtet werden.\n",
    "# Die helper-Funktion gibt oft schon die gekürzte Repräsentation zurück. Wir prüfen die Längen.\n",
    "\n",
    "print(\"Starte Datenbereinigung und Mapping...\")\n",
    "\n",
    "count_mismatch = 0\n",
    "for idx, (index, row) in enumerate(df_scop.iterrows()):\n",
    "    dssp_str = row['ss8_label']\n",
    "    \n",
    "    # Überspringe leere DSSP Ergebnisse\n",
    "    if not dssp_str or len(dssp_str) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Hole das Embedding für dieses Protein\n",
    "    # token_reps_trained ist eine Liste von Tensoren.\n",
    "    rep = token_reps_trained[idx] # Shape: (Seq_Len, Hidden_Dim)\n",
    "    \n",
    "    # Check lengths\n",
    "    # Manchmal enthalten Reps noch Start/End Tokens, das muss man prüfen.\n",
    "    # Hier nehmen wir an, rep hat die Länge der Sequenz.\n",
    "    if rep.shape[0] != len(dssp_str):\n",
    "        # Versuch: ESM fügt oft Start/End Token hinzu. \n",
    "        # Wenn rep = len(seq) + 2, schneiden wir ab.\n",
    "        if rep.shape[0] == len(dssp_str) + 2:\n",
    "            rep = rep[1:-1]\n",
    "        else:\n",
    "            count_mismatch += 1\n",
    "            continue\n",
    "\n",
    "    # Wenn Längen jetzt stimmen, Label encoden\n",
    "    try:\n",
    "        label_indices = [ss8_mapping.get(char, 7) for char in dssp_str] # 7 ist fallback für Coil\n",
    "        label_tensor = torch.tensor(label_indices, dtype=torch.long)\n",
    "        \n",
    "        # Speichern: (Embedding_Tensor, Label_Tensor, Original_Index)\n",
    "        cleaned_data.append({\n",
    "            'embedding': rep.cpu(), # Verschiebe in CPU RAM um GPU für Training freizuhalten\n",
    "            'label': label_tensor,\n",
    "            'df_index': index\n",
    "        })\n",
    "        valid_indices.append(index)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Bereinigung fertig.\")\n",
    "print(f\"Valid Samples: {len(cleaned_data)}\")\n",
    "print(f\"Verworfene Samples (Length Mismatch): {count_mismatch}\")\n",
    "\n",
    "# Erstelle ein Mapping von df_index zu Listen-Index in cleaned_data für schnellen Zugriff\n",
    "df_idx_to_list_idx = {item['df_index']: i for i, item in enumerate(cleaned_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Einfaches Lineares Modell (Logistic Regression in PyTorch)\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Dataset Wrapper für effizientes Laden\n",
    "class ProteinResidueDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        \"\"\"\n",
    "        data_list: Liste von Dicts {'embedding': tensor, 'label': tensor}\n",
    "        Wir flachen hier NICHT alles sofort ab, um Speicher zu sparen, \n",
    "        sondern geben pro Item ein Protein zurück.\n",
    "        Der DataLoader muss einen custom collate_fn nutzen um Batches zu bauen.\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]['embedding'], self.data[idx]['label']\n",
    "\n",
    "def collate_residues(batch):\n",
    "    # Stapelt alle Residues aus mehreren Proteinen in einen großen Tensor (Batch)\n",
    "    embeddings = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Concatenate along dimension 0\n",
    "    embeddings_cat = torch.cat(embeddings, dim=0)\n",
    "    labels_cat = torch.cat(labels, dim=0)\n",
    "    \n",
    "    return embeddings_cat, labels_cat\n",
    "\n",
    "def train_linear_probe(train_data, test_data, input_dim, device='cuda', epochs=5, batch_size=32):\n",
    "    model = LinearProbe(input_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_dataset = ProteinResidueDataset(train_data)\n",
    "    # Shuffle ist wichtig für Training\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_residues)\n",
    "    \n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    test_dataset = ProteinResidueDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_residues)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Durchführung des Experiments auf dem 'fold' Level (strengster Split)\n",
    "target_level = 'fold'\n",
    "folds = partitions[target_level]\n",
    "\n",
    "accuracies_trained = []\n",
    "# Optional: Hier könntest du auch accuracies_untrained tracken\n",
    "\n",
    "input_dim = cleaned_data[0]['embedding'].shape[1] # z.B. 320 oder 768\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Starte 5-Fold Cross Validation auf Level: {target_level}\")\n",
    "print(f\"Input Dimension: {input_dim}\")\n",
    "\n",
    "for fold_idx, (train_indices, test_indices) in enumerate(folds):\n",
    "    print(f\"\\n--- Fold {fold_idx + 1} / 5 ---\")\n",
    "    \n",
    "    # 1. Daten selektieren basierend auf den Indizes aus GroupKFold\n",
    "    # Wir müssen prüfen, ob die Indizes in unseren 'cleaned_data' noch existieren\n",
    "    train_subset = []\n",
    "    test_subset = []\n",
    "    \n",
    "    for idx in train_indices:\n",
    "        if idx in df_idx_to_list_idx:\n",
    "            train_subset.append(cleaned_data[df_idx_to_list_idx[idx]])\n",
    "            \n",
    "    for idx in test_indices:\n",
    "        if idx in df_idx_to_list_idx:\n",
    "            test_subset.append(cleaned_data[df_idx_to_list_idx[idx]])\n",
    "            \n",
    "    print(f\"Train samples: {len(train_subset)}, Test samples: {len(test_subset)}\")\n",
    "    \n",
    "    if len(train_subset) == 0 or len(test_subset) == 0:\n",
    "        print(\"Warnung: Leeres Set nach Filterung. Überspringe Fold.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Trainieren & Evaluieren\n",
    "    acc = train_linear_probe(train_subset, test_subset, input_dim, device=device, epochs=3)\n",
    "    accuracies_trained.append(acc)\n",
    "    print(f\"Fold {fold_idx + 1} Accuracy (Q8): {acc:.2f}%\")\n",
    "\n",
    "# %%\n",
    "# Ergebnis visualisieren\n",
    "mean_acc = np.mean(accuracies_trained)\n",
    "std_acc = np.std(accuracies_trained)\n",
    "\n",
    "print(\"\\nResultate:\")\n",
    "print(f\"Mean Q8 Accuracy: {mean_acc:.2f}% (+/- {std_acc:.2f})\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(['Trained ESM'], [mean_acc], yerr=[std_acc], capsize=10, color='skyblue', alpha=0.7)\n",
    "plt.ylabel('Q8 Accuracy (%)')\n",
    "plt.title(f'Secondary Structure Prediction (Linear Probe) - {target_level} split')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised-learning-in-biology (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
